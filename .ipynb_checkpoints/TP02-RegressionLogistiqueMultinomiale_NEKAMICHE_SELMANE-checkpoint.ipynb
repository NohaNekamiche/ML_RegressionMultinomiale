{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 : Régression logistique Multinomiale\n",
    "\n",
    "Binômes : \n",
    "- NOHA NEKAMICHE\n",
    "- LAMIA SELMANE\n",
    "\n",
    "\n",
    "**INTRODUCTION**\n",
    "\n",
    "Nous avons implémenté le cas d'une seule classe (binaire : oui ou non). Pour appliquer un classement sur plusieurs classes, on peut entrainner $L$ modèles de régression logistique (où $L$ est le nombre des classes). Dans ce cas, nos résultats (Y) doivent encodée en 0 et 1. Pour un modèle $M_i$ d'une classe $C_i$, la sortie $Y$ doit avoir 1 si $C_i$, 0 si une autre classe. (One-to-rest classification)\n",
    "\n",
    "Une autre approche (celle qu'on va implémenter) est d'encoder la sortie en utilisant OneHot encoder. Pour $L$ classes et un échantillon donnée, on va avoir $L$ sorties (une ayant 1 et les autres 0). Pour un dataset avec $M$ échantillons, $N$ caractéristiques et $L$ classes, on va avoir les dimensions suivantes : \n",
    "- $X [M, N]$\n",
    "- $Y [M, L]$\n",
    "- $\\theta [N, L]$\n",
    "\n",
    "Cette dernière approche s'appelle maximum entropy (MaxEnt). Elle généralise la régresion logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import outils\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension les algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, on va utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. Combinaison linéaire\n",
    "\n",
    "On combine les $N$  caractéristiques linéairement comme dans la régression linéaire binaire. \n",
    "La seule différence est que nous avons plus de classes, donc le nombre des paramètres va être multiplié par le nombre des classes.\n",
    "La somme pondérée d'une classe $c$ est calculée selon la formule : \n",
    "\n",
    "$$Z_c = zfn_c(X, \\theta) = \\sum\\limits_{j=0}^{N} \\theta_{(c, j)} X_j | X_0 = 1 $$\n",
    "\n",
    "La forme matricielle de $Z$ sera : \n",
    "$$Z = zfn(X, \\theta) = X \\cdot \\theta$$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $\\theta[N, L]$ : une matrice de N lignes (caractéristiques, y compris le biais) et L colonnes (classes). \n",
    "- $Z[M, L]$ : une matrice de M lignes (échantillons) et L colonnes (classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0.5, 0.1, 0.6],\n",
       "       [0.2, 0.3, 0. ],\n",
       "       [0.7, 0.4, 0.6]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO implémenter la fonction de combinaison linéaire \n",
    "def zfn(X, Theta): \n",
    "    return np.dot(X,Theta)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0. , 0. , 0. ],\n",
    "#        [0.5, 0.1, 0.6],\n",
    "#        [0.2, 0.3, 0. ],\n",
    "#        [0.7, 0.4, 0.6]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]]) # 4 échntillons, 2 caractéristiques\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "zfn(X_tn, Theta_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Calcul des probabilités\n",
    "\n",
    "Les valeurs combinées sont transformées à des probabilités en utilisant la fonction softmax. \n",
    "La fonction softmax nous assure que la somme des probabilités des classes égale à 1.\n",
    "Cette fonction prend les combinaisons linéaires $Z[M, L]$ et calcule les probabilités $P[M, L] comme suite : \n",
    "\n",
    "$$softmax(Z)=\\frac{e^Z}{\\sum\\limits_{k=1}^{L} e^{Z_k}}$$\n",
    "\n",
    "- $M$ nombre des échantillons\n",
    "- $N$ nombre des caractéristiques\n",
    "- $L$ nombre des classes\n",
    "- La somme des probabilités de chaque ligne doit être 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.33333333, 0.33333333, 0.33333333]),\n",
       " array([0.36029662, 0.24151404, 0.39818934]),\n",
       " array([0.34200877, 0.37797814, 0.28001309]),\n",
       " array([0.37797814, 0.28001309, 0.34200877])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction softmax\n",
    "def softmax(Z):\n",
    "    exp_z=np.exp(Z)\n",
    "    \n",
    "    v=[np.sum(exp_z[i]) for i in range(len(exp_z))]\n",
    "    \n",
    "    return [exp_z[i]/v[i] for i in range(len(v))]\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.33333333, 0.33333333, 0.33333333],\n",
    "#       [0.36029662, 0.24151404, 0.39818934],\n",
    "#       [0.34200877, 0.37797814, 0.28001309],\n",
    "#       [0.37797814, 0.28001309, 0.34200877]])\n",
    "#---------------------------------------------------------------------\n",
    "Z_tn = np.array([[0. , 0. , 0. ],\n",
    "                 [0.5, 0.1, 0.6],\n",
    "                 [0.2, 0.3, 0. ],\n",
    "                 [0.7, 0.4, 0.6]])\n",
    "softmax(Z_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Prédiction \n",
    "\n",
    "Etant donnée les probabilités des classes pour chaque échantillon, on doit choisir la classe avec le max de probabilité.\n",
    "\n",
    "$$\n",
    "\\hat{C}^{(i)}_j = \\begin{cases}\n",
    "1 & si & H^{(i)}_j \\ge \\max P^{(i)} \\\\\n",
    "0 & sinon & \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $H[M, L]$ probabilités où chaque ligne est yn échantillon et chaque collone est une classe\n",
    "- $\\hat{C}[M, L]$ prédictions où chaque ligne est yn échantillon et chaque collone est une classe. $\\hat{C}^{(i)}_j \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction de prédiction \n",
    "# Elle doit calculer la \n",
    "# H est un vecteur de probabilités \n",
    "def cn(H): \n",
    "      \n",
    "    res=np.zeros(np.shape(H))\n",
    "    for i_line,line in enumerate(H) :\n",
    "        for i_col,col in enumerate(line):\n",
    "            if(col >=np.max(line)):\n",
    "                res[i_line,i_col]=1\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 1, 1],\n",
    "#        [0, 0, 1],\n",
    "#        [0, 1, 0],\n",
    "#        [1, 0, 0]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "             [0.36029662, 0.24151404, 0.39818934],\n",
    "             [0.34200877, 0.37797814, 0.28001309],\n",
    "             [0.37797814, 0.28001309, 0.34200877]])\n",
    "cn(H_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Calcul du coût \n",
    "\n",
    "On réfère aux probabilités trouvées par la fonction softmax comme $H$, où $H_c$ est la probabilité d'une classe $c$.\n",
    "Etant donné un échantillon $X^{(i)}$, son coût est calculé comme : \n",
    "\n",
    "$$ cout(H^{(i)}, Y^{(i)}) = - \\sum\\limits_{c=1}^{L} Y^{(i)}_c \\log(H^{(i)}_c)$$\n",
    "\n",
    "Le coût total est la moyenne des coût de tous les échantillons\n",
    "\n",
    "$$J(H, Y) = \\frac{1}{M} \\sum\\limits_{i=1}^{M} cout(H^{(i)}, Y^{(i)})$$\n",
    "\n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1913194530574498"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction du coût multinomial\n",
    "def jn(H, Y):\n",
    "   \n",
    "    #cout=-np.sum(np.dot(np.log(H),Y.reshape(3,4)))\n",
    "    cout=-np.sum(Y*np.log(H))\n",
    "    return np.sum(cout)/Y.shape[0]\n",
    "   \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 1.1913194530574498\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "jn(H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Calcul des gradients\n",
    "\n",
    "La taille des gradients est la même que celle des paramètres $\\theta[N, L]$. \n",
    "\n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X^{(i)}_{j} $$\n",
    "\n",
    "Sa forme matricielle sera \n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} X^\\top \\cdot (H-Y) $$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)\n",
    "- $\\frac{\\partial J}{\\theta}[N, L]$ : une matrice de L lignes (classes) et N colonnes (caractéristiques, y compris le biais). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06543131, -0.11961822,  0.18504953],\n",
       "       [-0.07000327,  0.16449781, -0.09449454]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction du gradient multinomial\n",
    "def dJn(X, H, Y):\n",
    "    return np.dot(np.transpose(X),np.subtract(H,Y))/X.shape[0]\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-0.06543131, -0.11961822,  0.18504953],\n",
    "#        [-0.07000327,  0.16449781, -0.09449454]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.],\n",
    "                 [0., 1.],\n",
    "                 [1., 1.]])\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "dJn(X_tn, H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.6. Descente du gradient adaptative\n",
    "\n",
    "Les coéfficients sont mis à jour itérativement en se basant sur le gradient et un taux d'apprentissage $\\alpha$ comme dans la descente des gradients normale. La différence est que dans **AdaGrad**, on adapte le taux d'apprentissage de chaque paramètre $\\theta$ selon l'historique des gradients.\n",
    "\n",
    "\n",
    "#### I.6.1. Mise à jours des gradients\n",
    "\n",
    "Dans cette fonction, on va implémenter la désente des gradients normale (celle dans le TP précédent ... Hint: copy-coller)\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\theta_j}$$\n",
    "\n",
    "aussi, la déscente des gradients adaptative (AdaGrad)\n",
    "\n",
    "$$V = V + (\\frac{\\partial J(\\theta)}{\\partial \\theta})^2$$\n",
    "\n",
    "$$\\theta = \\theta - \\frac{\\alpha}{\\sqrt{V +\\epsilon}} \\frac{\\partial J(\\theta)}{\\theta}$$\n",
    "\n",
    "- $\\theta[N, L]$ : les paramètres de $L$ classes et $N$ caractéristiques\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}[N, L]$ les gradients de ces paramètres\n",
    "- $V[N, L]$ : hyper-paramètre pour l'adjustement du taux d'apprentissage pour chaque paramètre\n",
    "- $\\epsilon=e^{-8}$\n",
    "\n",
    "\n",
    "Quelques conditions sur la fonction :\n",
    "- La fonction doit retourner les paramètres mises-à-jours et le nouveau vecteur $V$\n",
    "- Si la valeur booléenne $adagrad=True$, on doit appliquer adagrad (calculer le nouveau V et mettre à jours $\\theta$ selon AdaGrad)\n",
    "- Si cette valeur égale à $False$, on utilise la mise à jour normale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[0.50065431, 0.10119618, 0.5981495 ],\n",
       "         [0.20070003, 0.29835502, 0.00094495]]), array([[0.2 , 0.1 , 0.5 ],\n",
       "         [0.05, 0.15, 0.2 ]])), (array([[0.50144768, 0.103538  , 0.59746826],\n",
       "         [0.20298765, 0.29609069, 0.00206732]]),\n",
       "  array([[0.20428126, 0.11430852, 0.53424333],\n",
       "         [0.05490046, 0.17705953, 0.20892922]])))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Définir la fonction de la mise à jours des paramètres avec AdaGrad\n",
    "def majThetaAdaGrad(Theta, Gradient, alpha, V, adagrad=False, eps=1e-08): \n",
    "\n",
    "    if(adagrad==False):\n",
    "        return Theta-alpha*Gradient,V\n",
    "    else:\n",
    "        V_new = V.copy()+np.square(Gradient)\n",
    "        Theta_new = Theta.copy()-(alpha*Gradient)/(V_new+eps)**0.5\n",
    "        return Theta_new, V_new\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ((array([[0.50065431, 0.10119618, 0.5981495 ],\n",
    "#          [0.20070003, 0.29835502, 0.00094495]]),\n",
    "#   array([[0.2 , 0.1 , 0.5 ],\n",
    "#          [0.05, 0.15, 0.2 ]])),\n",
    "#  (array([[0.50144768, 0.103538  , 0.59746826],\n",
    "#          [0.20298765, 0.29609069, 0.00206732]]),\n",
    "#   array([[0.20428126, 0.11430852, 0.53424333],\n",
    "#          [0.05490046, 0.17705953, 0.20892922]])))\n",
    "#---------------------------------------------------------------------\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "Gradient_tn = np.array([[-0.06543131, -0.11961822,  0.18504953], \n",
    "                       [-0.07000327,  0.16449781, -0.09449454]])# 2 caractéristiques, 3 classes\n",
    "V_tn = np.array([[0.2, 0.1, 0.5],\n",
    "                [0.05, 0.15, 0.2]]) # 2 caractéristiques, 3 classes\n",
    "alpha_tn = 0.01\n",
    "majThetaAdaGrad(Theta_tn, Gradient_tn, alpha_tn, V_tn), majThetaAdaGrad(Theta_tn, Gradient_tn, alpha_tn, V_tn, adagrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.6.2. La descente des gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.84298097,  1.57919742, -1.22217839],\n",
       "        [ 0.60036187, -1.45101777,  1.3506559 ]]), 0.5977646913907274)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def des_grad_adagrad(X, Y, Theta, max_iter=200, alpha=0.1, adagrad=False):\n",
    "    \n",
    "    couts = []\n",
    "    \n",
    "    V = np.zeros(Theta.shape) # Générer des zéros\n",
    "    Theta1 = Theta.copy() # pour ne pas modifier Theta original\n",
    "    \n",
    "    for i in range(max_iter): # Ici, la seule condition d'arrêt est le nombre des itérations\n",
    "        H = softmax(zfn(X, Theta1))\n",
    "        couts.append(jn(H, Y))\n",
    "        Theta1, V = majThetaAdaGrad(Theta1, dJn(X, H, Y), alpha, V, adagrad=adagrad)\n",
    "    \n",
    "    return Theta1, couts\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([[ 0.84298097,  1.57919742, -1.22217839],\n",
    "#         [ 0.60036187, -1.45101777,  1.3506559 ]]),\n",
    "#  0.5977646913907274)\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]]) # deux variables logiques\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]]) # égale, sup, inf, égale\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "\n",
    "theta_n, couts_n = des_grad_adagrad(X_tn, Y_tn, Theta_tn)\n",
    "\n",
    "theta_n, couts_n[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.7. Regrouper les fonctions ensemble \n",
    "\n",
    "Pour bien gérer l'entrainnement et la prédiction, on rassemble les fonctions que vous avez implémenté dans une seul classe. L'intérêt : \n",
    "- Si on applique la normalisation durant l'entrainnement, on doit l'appliquer aussi durant la prédiction. En plus, on doit utiliser les mêmes paramètres (moyenne et écart-type)\n",
    "- On utilise les thétas optimales lors de la prédicition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaxEnt(object):\n",
    "    \n",
    "    def __init__(self, norm=True, const=True): \n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "    \n",
    "    def entrainer(self, X, Y, max_iter=100, alpha=.01, adagrad=False): \n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const)\n",
    "        Theta = np.zeros((X_pre.shape[1], Y.shape[1])) # Theta[N, L]\n",
    "        self.Theta, self.couts = des_grad_adagrad(X_pre, Y, Theta, max_iter=max_iter, alpha=alpha, adagrad=adagrad)\n",
    "        \n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X, prob=True):\n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const, mean=self.mean, std=self.std)\n",
    "        H = softmax(zfn(X_pre, self.Theta))\n",
    "        if prob:\n",
    "            return H\n",
    "        return cn(H)\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 0, 1]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]]) # deux variables logiques\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]]) # égale, sup, inf, égale\n",
    "\n",
    "X_testn = np.array([[2., 2.], [1., 0.], [1., -1.], [2., 5.]])\n",
    "\n",
    "maxent = MaxEnt()\n",
    "maxent.entrainer(X_tn, Y_tn)\n",
    "maxent.predire(X_testn, prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Application et analyse\n",
    "\n",
    "On va utiliser [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) pour classer des fleurs en trois classes, en utilisant 4 caractéristiques. Pour simplification, on va utiliser seulement 2 caractéristiques: Petal Length (cm); Petal Width (cm). D'après [Ce tutoriel](https://teddykoker.com/2019/06/multi-class-classification-with-logistic-regression-in-python/) ces 2 caractéristiques sont suffisantes.\n",
    "\n",
    "**Dans cette partie, vous n'avez rien à programmer. Mais, il faut analyser les résultats à la fin**\n",
    "\n",
    "Deux solutions à analyser : \n",
    "- Entrainer 3 modèles de régression logistique binaire\n",
    "- Entrainer 1 modèle de régression logistique multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv(\"datasets/iris.csv\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width        class\n",
       "0           1.4          0.2  Iris-setosa\n",
       "1           1.4          0.2  Iris-setosa\n",
       "2           1.3          0.2  Iris-setosa\n",
       "3           1.5          0.2  Iris-setosa\n",
       "4           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if iris.shape[1] > 3:\n",
    "    iris.drop([\"sepal_length\", \"sepal_width\"], axis = 1, inplace=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Séparabilité des classes\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous concernant la séparabilité des 3 classes?\n",
    "- Donner une hypothèse concernant la performance des modèles sur ce dataset (Rappel, Précision)\n",
    "- Justifier cette hypothèse (Rappel, Précision) en comparant les 3 classes\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xiris = iris.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "Yiris = iris.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "setosa = iris[\"class\"] == \"Iris-setosa\"\n",
    "versicolor = iris[\"class\"] == \"Iris-versicolor\"\n",
    "virginica = iris[\"class\"] == \"Iris-virginica\"\n",
    "\n",
    "plt.scatter(Xiris[setosa, 0], Xiris[setosa, 1], color=\"red\", label=\"Iris-setosa\")\n",
    "plt.scatter(Xiris[versicolor, 0], Xiris[versicolor, 1], color=\"blue\", label=\"Iris-versicolor\")\n",
    "plt.scatter(Xiris[virginica, 0], Xiris[virginica, 1], color=\"green\", label=\"Iris-virginica\")\n",
    "\n",
    "plt.xlabel(\"sepal_length\")\n",
    "plt.ylabel(\"sepal_width\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remarques:\n",
    "#on a une  separation parfaite (linéairement separées) entre les points rouges(Iris-setosa) et bleu (Iris-versicolor) ainsi entre les points rouges \n",
    "#(Iris-setosa) et vert (Iris-virginica) dans ces deux cas on a une precision=1.0 et rappel=1.0 aussi entre\n",
    "#la majorité des points des bleu et vert\n",
    "#par contre on trouve quelque points bleu qui sont proche des points vert dans ce cas la precision va etre <1.0 et rappel=1.0\n",
    "#hypothèse:\n",
    "#si on augment le rappel seulement notre modèle ne sert pas à grand chose (pas une tres bonne separation entre les classes de données)\n",
    "#par contre si on ameliore aussi la precision  alors on diminue le risque de se tromper \n",
    "#justification d'hypothèses:\n",
    "#les classes sont linéairement separées parceque le modèle de regression multinomiale va etre capable de les separés.\n",
    "#la meme chose pour les modèles  de regression binaires,\n",
    "#car les points rouge(Iris-setosa) est tres eloiné des deux autres classes (donc elle a une precision et recall idea qui\n",
    "# egale à 1.0), par contre les deux autre classes sont proches dans quelque points dans ce cas on a une precision <1.0 de meme \n",
    "#pour recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "Xiris_train, Xiris_test, Yiris_train, Yiris_test = train_test_split(Xiris, Yiris, test_size=0.2, random_state=0)  \n",
    "\n",
    "len(Xiris_train), len(Xiris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. AdaGrad\n",
    "\n",
    "Nous avons entraîné deux modèles : \n",
    "- **DG** : modèle avec la déscente du gradient normale\n",
    "- **AdaGrad** : modèle avec AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_iris = OneHotEncoder()\n",
    "Yiris_train_enc = enc_iris.fit_transform(np.array(Yiris_train).reshape(-1,1))\n",
    "\n",
    "Yiris_train_enc.toarray()[:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1. Convergence\n",
    "\n",
    "Ici, on veut tester la convergence : rapidité et convergence finale (moins d'erreur = meilleur)\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse\n",
    "- Essayer de justifier cette hypothèse en se basant sur la formule de AdaGrad\n",
    "- Peut-on utiliser AdaGrad lorsque nous avons beaucoup de minimums locaux ? Pourquoi ?\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_ITER = 500\n",
    "mxent_dg = MaxEnt()\n",
    "mxent_dg.entrainer(Xiris_train, Yiris_train_enc.toarray(), max_iter=MAX_ITER)\n",
    "mxent_adagrad = MaxEnt()\n",
    "mxent_adagrad.entrainer(Xiris_train, Yiris_train_enc.toarray(), adagrad=True, max_iter=MAX_ITER)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(mxent_dg.couts, label = \"DG\")\n",
    "plt.plot(mxent_adagrad.couts, label = \"AdaGrad\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remarque:\n",
    "#on remarque avec descent gradient(DG) on a une meiulleur convergence (plus rapide ) qu'avec  AdaGrad \n",
    "#hypothèse:\n",
    "#l'utilisation de la methode Descent Gradient accelère la convergence de modèle ,contrairement à la methode AdaGrad \n",
    "#justification:\n",
    "#d'apres la formule de AdaGrad ,dans chaque iteration on sauvegarde les gradients cumulé dans V \n",
    "#donc la valeur de V augment dans chaque etape  ce qui implique une valeur menimale de thta dans chaque iteration\n",
    "#cela réduit le taux d'apprentissage et finit par devenir petit.\n",
    "#reponse:\n",
    "#oui Adagrad permet d'eviter les optimomes locaux ,justification: Adagrad deja fait pour regle ce \n",
    "#probleme par ce que c'etait le problème de DG ,car DANS Adagrad on a un taux d'apprentissage pour chaque iteration(theta)\n",
    "#Adagrad permet d'augmente le taux d'apprentissage de manière separé ceci permet le deplacement de chaque dimension de manière\n",
    "#propotionnel donc Adagrad va eviter les optimaux locaux ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1. Performance (AdaGrad)\n",
    "\n",
    "Ici, on veut tester la performance sur le dataset de test.\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous (Précision vs Recall (setosa); GD vs AdaGrad (micro-avg)) ?\n",
    "- Donner une hypothèse sur la convergence AdaGrad (peut-t-on converger) et comment l'améliorer\n",
    "- Essayer de justifier cette hypothèse (GD vs AdaGrad)\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descente du gradient\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       0.92      1.00      0.96        11\n",
      "Iris-versicolor       1.00      0.23      0.38        13\n",
      " Iris-virginica       0.40      1.00      0.57         6\n",
      "\n",
      "       accuracy                           0.67        30\n",
      "      macro avg       0.77      0.74      0.63        30\n",
      "   weighted avg       0.85      0.67      0.63        30\n",
      "\n",
      "AdaGrad\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       0.92      1.00      0.96        11\n",
      "Iris-versicolor       1.00      0.08      0.14        13\n",
      " Iris-virginica       0.35      1.00      0.52         6\n",
      "\n",
      "       accuracy                           0.60        30\n",
      "      macro avg       0.76      0.69      0.54        30\n",
      "   weighted avg       0.84      0.60      0.52        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Descente du gradient\")\n",
    "print(classification_report(Yiris_test, enc_iris.inverse_transform(mxent_dg.predire(Xiris_test)), target_names=enc_iris.categories_[0]))\n",
    "\n",
    "print(\"AdaGrad\")\n",
    "print(classification_report(Yiris_test, enc_iris.inverse_transform(mxent_adagrad.predire(Xiris_test)), target_names=enc_iris.categories_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMARQUES:\n",
    "#on remarque que la precision de Iris-setosa avec les deux methodes (DG et Adagrad ) est inferieur que rappel(recall),\n",
    "#aussi macro-avg de DG est meilleur que Adagrad\n",
    "#avec la methode Adagrad toujours on aura une convergence ,mais elle ne converge pas vers le minimum optimal (parceque en \n",
    "#Adagrad on a des gradients carrés accumulées dans le dénominateur).\n",
    "#pour l'amelioré : on définissons recursivement une moyenne décroissante de tous les gradients carées passées,donc la moyenne\n",
    "#mobile à chaque pâs de temps dépend uniquement de la moyenne précédente et du gradient actuel.\n",
    "#Justification d'hypothèse:\n",
    "#Adagrad est plus rapide  que DG en terme de temps parce qu'elle ce stabilise rapidement à cause de la mise à jour de vecteur V\n",
    "#dans chaque iteration mais la convergence finale n'est pas la plus petite contrairement au methode de DG ou on aura une \n",
    "#convergence meilleur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. One-vs-Rest OU One-vs-One\n",
    "\n",
    "Nous avons entrainé deux modèles : \n",
    "- **One-vs-Rest** : ici, trois sous-modèles binaires sont entraînés ; un pour chaque class. Chaque sous modèle détecte si l'échantillon appartient à sa classe ou non. Lors de la prédiction, on prend la classe avec le max de probabilité\n",
    "- **One-vs-One** : ici, un modèle de régression logistique multinomiale (maximum entropy) est entraîné pour séparer les trois classes\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "On remarque que la performance de One-vs-One est meilleure que celle de One-vs-Rest\n",
    "- Pourquoi ? (en se basant sur la limite de décision et les paramètres)\n",
    "- Quel mécanisme de ces deux est affecté beaucoup plus par les valeurs aberrantes (les échantillons d'une classe qui peuvent se retrouver aux milieu d'une autre classe)\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-Rest\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       0.93      1.00      0.96        13\n",
      " Iris-virginica       1.00      0.83      0.91         6\n",
      "\n",
      "       accuracy                           0.97        30\n",
      "      macro avg       0.98      0.94      0.96        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "One-vs-One\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       1.00      1.00      1.00        13\n",
      " Iris-virginica       1.00      1.00      1.00         6\n",
      "\n",
      "       accuracy                           1.00        30\n",
      "      macro avg       1.00      1.00      1.00        30\n",
      "   weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "one2rest = LogisticRegression(solver=\"lbfgs\", penalty=\"none\", multi_class=\"ovr\")\n",
    "one2rest.fit(Xiris_train, Yiris_train)\n",
    "\n",
    "one2one = LogisticRegression(solver=\"lbfgs\", penalty=\"none\", multi_class=\"multinomial\")\n",
    "one2one.fit(Xiris_train, Yiris_train)\n",
    "\n",
    "print(\"One-vs-Rest\")\n",
    "print(classification_report(Yiris_test, one2rest.predict(Xiris_test)))\n",
    "\n",
    "print(\"One-vs-One\")\n",
    "print(classification_report(Yiris_test, one2one.predict(Xiris_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réponses :\n",
    "#Pourquoi ?\n",
    "#le modèle One-vs-One mielleur que One-vs-Rest parcequ'il est basé sur la reduction des paramètres generiques \n",
    "# et la combinaison de certains d'etre eux ,car One-vs-One divise un ensemble de données de classification multi-classes \n",
    "#en problèmes de classification binaire . Contrairement à One-vs-Rest qui le divise en un ensemble de données binaire\n",
    "#pour chaque classe, l'approche One-vs-One divise l'ensemble de données en un ensemble de données pour chaque\n",
    "#classe par rapport à toutes les autres classes.\n",
    "#On trouve généralement le problème de déséquilibre des classes dans l'approche One-vs-Rest car on prend deux classes\n",
    "#et une classe a part, par contre dans l'approche On-vs-One on trouve pas ce problème.\n",
    "#De  plus dans One-vs-Rest, est basée sur la création de plusieurs classes qui engendre par conséquent un processus\n",
    "#tres lent à exécuter, par contre dans One-vs-One est basé sur la séparation des trois classes (il est divisé sur deux)\n",
    "#Donc plus de performance\n",
    "#Le mécanisme a affecter pour les valeurs aberrantes est : One-vs-Rest\n",
    "#Car cette approche est basée sur la création de plusieurs classes et les sous modèles binaires sont entraînés un pour\n",
    "#chaque classe avec le max de probabilité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonne chance\n"
     ]
    }
   ],
   "source": [
    "print(\"Bonne chance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
